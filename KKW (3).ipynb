{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b32c75-c6a3-424e-a037-2ad2c00b1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# House Price using Linear Regression \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "gharDF = pd.read_csv('USA_Housing.csv')\n",
    "gharDF.head()\n",
    "\n",
    "gharDF.info()\n",
    "\n",
    "gharDF.describe()\n",
    "\n",
    "gharDF.columns\n",
    "\n",
    "sns.pairplot(gharDF)\n",
    "\n",
    "sns.distplot(gharDF['Price'])\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = gharDF.select_dtypes(include='number')\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap (Numeric Features Only)\")\n",
    "plt.show()\n",
    "\n",
    "X = gharDF[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',\n",
    "               'Avg. Area Number of Bedrooms', 'Area Population']]\n",
    "\n",
    "y = gharDF['Price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm =LinearRegression()\n",
    "\n",
    "lm.fit(X_train,y_train)\n",
    "\n",
    "print(lm.intercept_)\n",
    "\n",
    "coeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])\n",
    "coeff_df\n",
    "\n",
    "predictions = lm.predict(X_test)  \n",
    "plt.scatter(y_test,predictions)\n",
    "\n",
    "sns.distplot((y_test-predictions),bins=50);\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
    "\n",
    "\n",
    "# or \n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Load the dataset from the provided URL\n",
    "url = \"https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-PricePrediction/master/USA_Housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 2: Display first few rows to understand the structure\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 3: Check for null values and data types\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Step 4: Statistical summary of numeric features\n",
    "print(\"\\nStatistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Step 5: Visualize correlations using a heatmap\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap of Numerical Features\")\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Define features (X) and target (y)\n",
    "X = df.drop(columns=['Price', 'Address'])  # Remove target and non-numeric feature\n",
    "y = df['Price']\n",
    "\n",
    "# Step 7: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Create and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 9: Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 10: Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nModel Evaluation:\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"R-squared Score (R2): {r2:.2f}\")\n",
    "\n",
    "# Step 11: Visualize Actual vs Predicted prices\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, color='blue')\n",
    "plt.xlabel(\"Actual House Prices\")\n",
    "plt.ylabel(\"Predicted House Prices\")\n",
    "plt.title(\"Actual vs Predicted House Prices\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd17aa99-5e8b-4ee9-93be-3df38406f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi class classifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to [0,1]\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "\n",
    "# Reshape input to add channel dimension\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train_cat = to_categorical(y_train, 10)\n",
    "y_test_cat = to_categorical(y_test, 10)\n",
    "\n",
    "# Step 2: Define CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # 10 output classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train_cat, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Step 3: Evaluate the Model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b7af2e-ad49-4500-bb21-9cbabcdec199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM \n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Load Dataset (only the top 10,000 words)\n",
    "vocab_size = 10000\n",
    "maxlen = 200  # maximum review length\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# 2. Preprocess - Padding to ensure equal input length\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "# 3. Build LSTM Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=maxlen))\n",
    "model.add(LSTM(64, return_sequences=False))  # or GRU(64)\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 4. Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 5. Train Model\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 6. Evaluate Model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 7. Confusion Matrix\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 8. Classification Report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1280e6b-d4c2-4543-9910-f2bb29f06421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for image classification \n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load and normalize data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train/255.0, x_test/255.0\n",
    "\n",
    "\n",
    "# Build simple CNN\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "# Predict some test images\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Show 5 images with predicted labels\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,2))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.xticks([]); plt.yticks([])\n",
    "    plt.imshow(x_test[i])\n",
    "    plt.xlabel(class_names[np.argmax(predictions[i])])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d54addc-27ae-42f7-9056-041f17fe90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load and normalize data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Define class names\n",
    "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "# Build CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32,32,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "\n",
    "# Plot training and validation accuracy/loss\n",
    "def plot_history(hist):\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(hist.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(hist.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(hist.history['loss'], label='Train Loss')\n",
    "    plt.plot(hist.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "predictions = model.predict(x_test)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\")\n",
    "print(classification_report(y_test, predicted_labels, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# Show predictions vs actuals\n",
    "def plot_predictions(images, labels, preds, class_names, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.xticks([]); plt.yticks([])\n",
    "        plt.imshow(images[i])\n",
    "        true_label = class_names[labels[i]]\n",
    "        pred_label = class_names[preds[i]]\n",
    "        color = 'green' if labels[i] == preds[i] else 'red'\n",
    "        plt.xlabel(f\"Pred: {pred_label}\\nTrue: {true_label}\", color=color)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot first 10 test images with prediction\n",
    "plot_predictions(x_test[:10], y_test[:10], predicted_labels[:10], class_names)\n",
    "\n",
    "# Show 10 random predictions\n",
    "rand_idxs = np.random.choice(len(x_test), size=10, replace=False)\n",
    "plot_predictions(x_test[rand_idxs], y_test[rand_idxs], predicted_labels[rand_idxs], class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fde2a-f0b7-4328-911d-64af5cc15679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analysis of Network Graph \n",
    "# Install necessary libraries\n",
    "!pip install tensorflow networkx matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example Data: You could replace this with real network data and text associated with nodes\n",
    "text_data = [\"I love this!\", \"I hate this.\", \"This is amazing.\", \"This is bad.\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "\n",
    "# Preprocessing Text Data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "X = tokenizer.texts_to_sequences(text_data)\n",
    "X = pad_sequences(X, padding='post', maxlen=10)\n",
    "\n",
    "# Create the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=10000, output_dim=128, input_length=10))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the data\n",
    "model.fit(X, np.array(labels), epochs=10, batch_size=2)\n",
    "\n",
    "# Sentiment prediction\n",
    "sentiments = model.predict(X)\n",
    "\n",
    "# Create a Network Graph (Example)\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for idx, text in enumerate(text_data):\n",
    "    G.add_node(idx, label=text, sentiment=sentiments[idx][0])\n",
    "\n",
    "# Add some edges between the nodes (Example)\n",
    "G.add_edges_from([(0, 1), (1, 2), (2, 3)])\n",
    "\n",
    "# Visualizing the Network Graph with Sentiment Data\n",
    "node_colors = ['green' if G.nodes[node]['sentiment'] > 0.5 else 'red' for node in G.nodes]\n",
    "node_labels = {node: f\"{G.nodes[node]['label']} \\n Sentiment: {G.nodes[node]['sentiment']:.2f}\" for node in G.nodes}\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "nx.draw(G, with_labels=True, node_size=3000, node_color=node_colors, font_size=12, font_weight='bold', font_color='white')\n",
    "plt.title('Network Graph with Sentiment Analysis')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# or \n",
    "6. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Load and merge data\n",
    "nodes = pd.read_csv('nodes.csv')\n",
    "edges = pd.read_csv('edges.csv')\n",
    "labels = pd.read_csv('labels.csv')\n",
    "\n",
    "# Merge nodes with labels using user_id as key\n",
    "data = pd.merge(nodes, labels, on='user_id')\n",
    "\n",
    "# Create network graph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(data['user_id'])\n",
    "G.add_edges_from([(row['user_id1'], row['user_id2']) for _, row in edges.iterrows()])\n",
    "\n",
    "# Visualize subset of graph for better clarity\n",
    "plt.figure(figsize=(10, 8))\n",
    "nx.draw(G.subgraph(list(data['user_id'])[:200]),  # Show first 200 nodes for clarity\n",
    "        with_labels=False, \n",
    "        node_size=40,\n",
    "        node_color=data['label'][:200].map({0:'red', 1:'green'}),\n",
    "        alpha=0.7)\n",
    "plt.title(\"Network Graph (Red=Negative, Green=Positive)\")\n",
    "plt.show()\n",
    "\n",
    "# Text preprocessing\n",
    "tokenizer = Tokenizer(num_words=2000, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "sequences = tokenizer.texts_to_sequences(data['text'])\n",
    "X = pad_sequences(sequences, maxlen=15, padding='post', truncating='post')\n",
    "\n",
    "# Labels\n",
    "y = data['label'].values\n",
    "\n",
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Maintain class balance\n",
    ")\n",
    "\n",
    "# Enhanced RNN model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=2000, output_dim=64, input_length=15),\n",
    "    SimpleRNN(64, return_sequences=True),\n",
    "    Dropout(0.5),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', \n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train with early stopping\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    validation_split=0.2,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Precision: {test_precision:.2f}\")\n",
    "print(f\"Recall: {test_recall:.2f}\")\n",
    "\n",
    "# Prediction function\n",
    "def predict_sentiment(text):\n",
    "    sample_seq = tokenizer.texts_to_sequences([text])\n",
    "    sample_padded = pad_sequences(sample_seq, maxlen=15, padding='post', truncating='post')\n",
    "    prediction = model.predict(sample_padded)\n",
    "    return 'Positive' if prediction[0][0] > 0.5 else 'Negative'\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nSample Predictions:\")\n",
    "print(\"I love this product! ->\", predict_sentiment(\"I love this product!\"))\n",
    "print(\"Terrible service ->\", predict_sentiment(\"Terrible service\"))\n",
    "print(\"It's okay, not great ->\", predict_sentiment(\"It's okay, not great\"))\n",
    "\n",
    "\n",
    "\n",
    "# or \n",
    "\n",
    "# 1. Import Libraries\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "import numpy as np\n",
    "\n",
    "# 2. Create small training data\n",
    "texts = [\n",
    "    \"I love this movie\", \"What an amazing performance\",\n",
    "    \"Fantastic visuals and story\", \"I hated this movie\", \"Terrible experience\", \"Worst acting ever\"\n",
    "]\n",
    "labels = [1, 1, 1, 0, 0, 0]  # 1=Positive, 0=Negative\n",
    "\n",
    "# 3. Tokenize and pad\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X_train = pad_sequences(sequences, maxlen=10)\n",
    "\n",
    "# 4. Build small RNN\n",
    "model = Sequential([\n",
    "    Embedding(1000, 16, input_length=10),\n",
    "    SimpleRNN(16),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, np.array(labels), epochs=15, batch_size=2)\n",
    "\n",
    "# 5. Create Graph with New Texts\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(0,1), (1,2), (1,3), (2,4)])\n",
    "\n",
    "node_texts = {\n",
    "    0: \"I really enjoyed the movie\",\n",
    "    1: \"The movie was very bad and boring\",\n",
    "    2: \"What an amazing experience\",\n",
    "    3: \"Terrible storyline and poor acting\",\n",
    "    4: \"Fantastic visuals and acting\"\n",
    "}\n",
    "\n",
    "# 6. Tokenize node texts\n",
    "node_sequences = tokenizer.texts_to_sequences(node_texts.values())\n",
    "X_nodes = pad_sequences(node_sequences, maxlen=10)\n",
    "\n",
    "# 7. Predict sentiments\n",
    "predictions = model.predict(X_nodes)\n",
    "\n",
    "# 8. Color nodes\n",
    "node_colors = ['green' if p > 0.5 else 'red' for p in predictions]\n",
    "\n",
    "\n",
    "# 9. Draw graph\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=800, font_color='white')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3885afb-33f8-47a1-9d27-fcf5beb80301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data from different Sources such as (Excel, Sql Server, Oracle etc.) and load in \n",
    "#targeted system.\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Step 1: Load Excel or fallback to sample data\n",
    "excel_file = 'data.xlsx'\n",
    "\n",
    "if os.path.exists(excel_file):\n",
    "    df = pd.read_excel(excel_file, sheet_name='Sheet1')\n",
    "    print(\"✅ Excel file loaded successfully!\\n\")\n",
    "else:\n",
    "    print(f\"⚠️ Excel file '{excel_file}' not found. Using sample data instead.\")\n",
    "    df = pd.DataFrame({\n",
    "        'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "        'Age': [25, 30, 22, 35],\n",
    "        'City': ['Nashik', 'Mumbai', 'Pune', 'Nagpur']\n",
    "    })\n",
    "\n",
    "# Step 2: Connect to SQLite\n",
    "conn = sqlite3.connect('imported_data.db')\n",
    "print(\"\\n🔌 Connected to SQLite database.\")\n",
    "\n",
    "# Step 3: Save DataFrame to SQL\n",
    "table_name = 'ImportedData'\n",
    "df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "print(f\"\\n📥 Data loaded into SQLite table: '{table_name}'\")\n",
    "\n",
    "# Step 4: Read back from SQL to verify\n",
    "print(\"\\n🔍 Preview from SQLite DB:\")\n",
    "print(pd.read_sql(f\"SELECT * FROM {table_name} LIMIT 5\", conn))\n",
    "\n",
    "# Step 5: Export to CSV\n",
    "csv_file = 'exported_data.csv'\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"\\n📤 Data exported to CSV as '{csv_file}'\")\n",
    "\n",
    "# Final confirmation\n",
    "print(\"\\n✅ Process completed successfully — Excel/Sample → SQLite → CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7e5ad-46cb-41f7-b7d2-607ff851765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Visualization from Extraction Transformation and Loading (ETL) Process \n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----------------------\n",
    "# ETL PROCESS\n",
    "# ----------------------\n",
    "\n",
    "# 1. EXTRACT: Get data from Excel\n",
    "def extract_excel(file_path):\n",
    "    \"\"\"Extracts data from Excel file\n",
    "    Args:\n",
    "        file_path (str): Path to Excel file\n",
    "    Returns:\n",
    "        DataFrame: Pandas DataFrame with extracted data\n",
    "    \"\"\"\n",
    "    return pd.read_excel(file_path)\n",
    "\n",
    "# 2. TRANSFORM: Clean and preprocess data\n",
    "def transform_data(df):\n",
    "    \"\"\"Cleans the extracted DataFrame\n",
    "    Args:\n",
    "        df (DataFrame): Raw DataFrame\n",
    "    Returns:\n",
    "        DataFrame: Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    df.dropna(inplace=True)  # Remove missing values\n",
    "    df.drop_duplicates(inplace=True)  # Remove duplicates\n",
    "\n",
    "    if 'order_date' in df.columns:\n",
    "        df['order_date'] = pd.to_datetime(df['order_date'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# 3. LOAD: Save cleaned data\n",
    "def load_data(df, target='output.csv'):\n",
    "    \"\"\"Saves data to a CSV file\n",
    "    Args:\n",
    "        df (DataFrame): Cleaned data\n",
    "        target (str): Output CSV filename\n",
    "    \"\"\"\n",
    "    df.to_csv(target, index=False)\n",
    "\n",
    "# ----------------------\n",
    "# DATA VISUALIZATION\n",
    "# ----------------------\n",
    "def visualize_data(df):\n",
    "    \"\"\"Creates visualizations from processed data\n",
    "    Args:\n",
    "        df (DataFrame): Processed BI data\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Sales Trend Visualization\n",
    "    if 'order_date' in df.columns and 'sales' in df.columns:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.lineplot(data=df, x='order_date', y='sales', estimator='sum')\n",
    "        plt.title('Monthly Sales Trend')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    # Product Performance Visualization\n",
    "    if 'product_category' in df.columns and 'sales' in df.columns:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.barplot(data=df, x='product_category', y='sales', estimator=sum)\n",
    "        plt.title('Sales by Product Category')\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('sales_analysis.png')\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# MAIN WORKFLOW\n",
    "# ----------------------\n",
    "def main():\n",
    "    try:\n",
    "        # EXTRACT PHASE\n",
    "        excel_data = extract_excel('sales_data.xlsx')\n",
    "\n",
    "        # TRANSFORM PHASE\n",
    "        cleaned_data = transform_data(excel_data)\n",
    "\n",
    "        # LOAD PHASE\n",
    "        load_data(cleaned_data, 'bi_output.csv')\n",
    "\n",
    "        # VISUALIZATION PHASE\n",
    "        visualize_data(cleaned_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ETL Process Failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f382e39a-ce4c-4715-a74d-ee355cb0ad7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data and load in targeted system \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from datetime import datetime\n",
    "\n",
    "# --- EXTRACT ---\n",
    "# Load Excel data (use your file path)\n",
    "df = pd.read_excel(\"sales_data.xlsx\")\n",
    "\n",
    "# --- TRANSFORM ---\n",
    "# Clean column names\n",
    "df.columns = [col.strip().lower().replace(\" \", \"_\") for col in df.columns]\n",
    "\n",
    "# Drop rows with missing critical values\n",
    "df.dropna(subset=['price', 'quantity'], inplace=True)\n",
    "\n",
    "# Add new calculated column\n",
    "df['revenue'] = df['price'] * df['quantity']\n",
    "\n",
    "# Convert date column to datetime (if applicable)\n",
    "if 'date' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "# Fill missing cities with 'Unknown'\n",
    "if 'city' in df.columns:\n",
    "    df['city'] = df['city'].fillna('Unknown')\n",
    "\n",
    "# --- LOAD ---\n",
    "# Define connection string (replace with your actual credentials & server)\n",
    "connection_string = \"mssql+pyodbc://username:password@your_server/your_database?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = sqlalchemy.create_engine(connection_string)\n",
    "\n",
    "# Write the DataFrame to SQL Server table\n",
    "df.to_sql('sales_data', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"✅ ETL Process Complete: Excel → Transformed → SQL Server\")\n",
    "\n",
    "\n",
    "\n",
    "''' ✅ Power BI ETL (Power Query M Script)\n",
    "To use this:\n",
    "\n",
    "Open Power BI Desktop\n",
    "\n",
    "Go to Home > Transform Data\n",
    "\n",
    "In Power Query, go to Advanced Editor\n",
    "\n",
    "Paste the below code\n",
    "\n",
    "'''\n",
    "\n",
    "let\n",
    "    // --- EXTRACT ---\n",
    "    Source = Excel.Workbook(File.Contents(\"C:\\Users\\YourName\\Documents\\sales_data.xlsx\"), null, true),\n",
    "    Sheet = Source{[Item=\"Sheet1\",Kind=\"Sheet\"]}[Data],\n",
    "    PromotedHeaders = Table.PromoteHeaders(Sheet, [PromoteAllScalars=true]),\n",
    "\n",
    "    // --- TRANSFORM ---\n",
    "    RenamedCols = Table.RenameColumns(PromotedHeaders, {\n",
    "        {\"Price\", \"price\"},\n",
    "        {\"Quantity\", \"quantity\"},\n",
    "        {\"City\", \"city\"},\n",
    "        {\"Date\", \"date\"}\n",
    "    }),\n",
    "    ChangedTypes = Table.TransformColumnTypes(RenamedCols, {\n",
    "        {\"price\", type number},\n",
    "        {\"quantity\", Int64.Type},\n",
    "        {\"city\", type text},\n",
    "        {\"date\", type datetime}\n",
    "    }),\n",
    "    CleanedCity = Table.ReplaceValue(ChangedTypes, null, \"Unknown\", Replacer.ReplaceValue, {\"city\"}),\n",
    "\n",
    "    // Add Calculated Column: revenue = price * quantity\n",
    "    AddedRevenue = Table.AddColumn(CleanedCity, \"revenue\", each [price] * [quantity], type number)\n",
    "\n",
    "    // --- LOAD ---\n",
    "    // At this point, Power BI will load the data into the data model when you click \"Close & Apply\"\n",
    "in\n",
    "    AddedRevenue\n",
    "\n",
    "\n",
    "'''\n",
    "Import the legacy data from different sources such as ( Excel , SqlServer, \n",
    "Oracle etc.) and load in the target system. ( You can download sample \n",
    "database such as Adventureworks, Northwind, foodmart etc.)  \n",
    "Step 1: Open Power BI \n",
    "Step 2: Click on Get data following list will be displayed → select Excel \n",
    "Step 3: Select required file and click on Open, Navigator screen appears \n",
    "Step 4: Select file and click on edit \n",
    ". \n",
    "Step 5: Power query editor appears \n",
    "Step 6: Again, go to Get Data and select OData feed \n",
    "Step 7: \n",
    "Paste url as http://services.odata.org/V3/Northwind/Northwind.svc/ \n",
    "Click on ok \n",
    "Step 8: Select orders table \n",
    "And click on edit \n",
    "Note: If you just want to see preview you can just'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd97b47-9b13-4025-9c99-f8926364a298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Excel \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 1: Load Excel Data\n",
    "# ----------------------------------------\n",
    "\n",
    "# Read all sheets from Excel\n",
    "excel_file = \"sales_data.xlsx\"\n",
    "sales_df = pd.read_excel(excel_file, sheet_name=\"Sales\")\n",
    "customers_df = pd.read_excel(excel_file, sheet_name=\"Customers\")\n",
    "products_df = pd.read_excel(excel_file, sheet_name=\"Products\")\n",
    "\n",
    "print(\"Sales Data:\")\n",
    "display(sales_df.head())\n",
    "\n",
    "print(\"\\nCustomers Data:\")\n",
    "display(customers_df.head())\n",
    "\n",
    "print(\"\\nProducts Data:\")\n",
    "display(products_df.head())\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 2: Data Cleaning (Excel-like \"Format as Table\")\n",
    "# ----------------------------------------\n",
    "\n",
    "# Remove duplicates\n",
    "sales_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Handle missing values (e.g., fill with 0)\n",
    "sales_df[\"Quantity\"].fillna(0, inplace=True)\n",
    "\n",
    "# Convert OrderDate to datetime\n",
    "sales_df[\"OrderDate\"] = pd.to_datetime(sales_df[\"OrderDate\"])\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 3: Advanced Excel Formulas (VLOOKUP, SUMIF)\n",
    "# ----------------------------------------\n",
    "\n",
    "# VLOOKUP: Merge Customer Name into Sales Data\n",
    "sales_df = pd.merge(sales_df, customers_df, on=\"CustomerID\", how=\"left\")\n",
    "\n",
    "# VLOOKUP: Merge Product Price into Sales Data\n",
    "sales_df = pd.merge(sales_df, products_df[[\"Product\", \"Price\"]], on=\"Product\", how=\"left\")\n",
    "\n",
    "# Verify the columns after merge\n",
    "print(\"Columns in sales_df after merge:\")\n",
    "print(sales_df.columns)\n",
    "\n",
    "# Check if 'Region' exists\n",
    "if 'Region' not in sales_df.columns:\n",
    "    print(\"The 'Region' column is missing after merge.\")\n",
    "else:\n",
    "    print(\"The 'Region' column exists.\")\n",
    "\n",
    "# SUMIF: Calculate Total Revenue (Quantity * Price)\n",
    "if 'Price' in sales_df.columns:\n",
    "    sales_df[\"TotalRevenue\"] = sales_df[\"Quantity\"] * sales_df[\"Price\"]\n",
    "else:\n",
    "    print(\"Price column is missing after merge.\")\n",
    "\n",
    "print(\"\\nMerged Data with Revenue:\")\n",
    "display(sales_df.head())\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 4: PivotTables (Excel-like Summarization)\n",
    "# ----------------------------------------\n",
    "\n",
    "# Check for presence of 'Region' before creating PivotTable\n",
    "if 'Region' in sales_df.columns:\n",
    "    # Create a PivotTable: Total Revenue by Region and Product\n",
    "    pivot_table = pd.pivot_table(\n",
    "        sales_df,\n",
    "        values=\"TotalRevenue\",\n",
    "        index=\"Region\",\n",
    "        columns=\"Product\",\n",
    "        aggfunc=\"sum\",\n",
    "        fill_value=0\n",
    "    )\n",
    "    print(\"\\nPivotTable (Revenue by Region & Product):\")\n",
    "    display(pivot_table)\n",
    "else:\n",
    "    print(\"'Region' column not found. Cannot create PivotTable.\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 5: Visualization (Excel-like Charts)\n",
    "# ----------------------------------------\n",
    "\n",
    "# Bar Chart: Total Revenue by Product\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=sales_df, x=\"Product\", y=\"TotalRevenue\", estimator=sum, ci=None)\n",
    "plt.title(\"Total Revenue by Product (Excel-like Bar Chart)\")\n",
    "plt.xlabel(\"Product\")\n",
    "plt.ylabel(\"Total Revenue ($)\")\n",
    "plt.show()\n",
    "\n",
    "# Line Chart: Monthly Sales Trend\n",
    "sales_df[\"Month\"] = sales_df[\"OrderDate\"].dt.month_name()\n",
    "monthly_sales = sales_df.groupby(\"Month\")[\"TotalRevenue\"].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=monthly_sales, x=\"Month\", y=\"TotalRevenue\", marker=\"o\")\n",
    "plt.title(\"Monthly Sales Trend (Excel-like Line Chart)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Total Revenue ($)\")\n",
    "plt.show()\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 6: Export Results to Excel\n",
    "# ----------------------------------------\n",
    "\n",
    "with pd.ExcelWriter(\"analysis_results.xlsx\") as writer:\n",
    "    sales_df.to_excel(writer, sheet_name=\"Processed Sales\", index=False)\n",
    "    if 'Region' in sales_df.columns:\n",
    "        pivot_table.to_excel(writer, sheet_name=\"PivotTable\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f777a3-9500-433f-839f-020ed2dbf86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression \n",
    "# ----------------------------------------\n",
    "# Import Libraries\n",
    "# ----------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 1: Load and Explore Data\n",
    "# ----------------------------------------\n",
    "df = pd.read_csv(\"customer_churn_large.csv\")\n",
    "print(\"Dataset Head:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass Distribution (Churn):\")\n",
    "print(df[\"Churn\"].value_counts())\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 2: Data Preprocessing\n",
    "# ----------------------------------------\n",
    "# Drop irrelevant columns\n",
    "if 'customerID' in df.columns:\n",
    "    df = df.drop(\"customerID\", axis=1)\n",
    "\n",
    "# Convert 'TotalCharges' to numeric (handle non-numeric values if any)\n",
    "df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
    "\n",
    "# Handle missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert categorical columns to numeric using one-hot encoding\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Split features (X) and target (y)\n",
    "X = df.drop(\"Churn_Yes\", axis=1)\n",
    "y = df[\"Churn_Yes\"]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 3: Train the Model\n",
    "# ----------------------------------------\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Step 4: Evaluate the Model\n",
    "# ----------------------------------------\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e712054b-2b5f-4378-9da3-882c5cef4d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering ALgo \n",
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate Sample Data\n",
    "X, y = make_blobs(n_samples=300, centers=3, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Visualize the Data\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "plt.title(\"Raw Data\")\n",
    "plt.show()\n",
    "\n",
    "# Find Optimal Clusters Using the Elbow Method\n",
    "inertia = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()\n",
    "\n",
    "# Apply K-means with Optimal Clusters (k=3)\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
    "kmeans.fit(X)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Visualize Clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "plt.title(\"K-means Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94caf12-f66a-429a-86b8-60a2352c0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# House Price Linear regression \n",
    "# Step 1: Import the necessary libraries\n",
    "# Import essential libraries for data handling, model building, and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 2: Create a multi-feature dataset (House Prices)\n",
    "# Simulate a dataset with house attributes and corresponding prices\n",
    "data = {\n",
    "    'Square_Feet': [1000, 1500, 1800, 2400, 3000, 3500],\n",
    "    'Bedrooms': [2, 3, 3, 4, 4, 5],\n",
    "    'Bathrooms': [1, 2, 2, 3, 3, 4],\n",
    "    'Price': [50000, 70000, 80000, 110000, 150000, 180000]\n",
    "}\n",
    "\n",
    "# Step 3: Create a DataFrame\n",
    "# Convert the dataset into a structured DataFrame for analysis\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 4: Define the features (X) and target (y)\n",
    "# Separate the DataFrame into independent features (X) and the dependent variable (y)\n",
    "X = df[['Square_Feet', 'Bedrooms', 'Bathrooms']]  # Features\n",
    "y = df['Price']  # Target variable\n",
    "\n",
    "# Step 5: Split the dataset into training and testing sets\n",
    "# Use train_test_split to create training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Create a linear regression model\n",
    "# Initialize the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Step 7: Train the model on the training data\n",
    "# Fit the linear regression model using the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make predictions using the model\n",
    "# Use the trained model to predict the target variable for the testing set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate the model\n",
    "# Calculate evaluation metrics for the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared (R2): {r2}\")\n",
    "\n",
    "# Step 10: Visualize Feature Importance\n",
    "# Retrieve model coefficients to understand the impact of each feature\n",
    "coefficients = model.coef_\n",
    "features = X.columns\n",
    "\n",
    "# Colorful bar chart for feature importance\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=features, y=coefficients, palette='viridis')\n",
    "plt.title('Feature Importance (Model Coefficients)', fontsize=16)\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Coefficient Value', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Step 11: Visualize Actual vs Predicted Prices\n",
    "# Scatter plot for Actual vs Predicted prices\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_test, y=y_pred, color='purple', s=100, alpha=0.8, label='Predicted vs Actual')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', label='Perfect Prediction Line')\n",
    "plt.xlabel('Actual Prices', fontsize=12)\n",
    "plt.ylabel('Predicted Prices', fontsize=12)\n",
    "plt.title('Actual vs Predicted Prices', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.6)\n",
    "plt.show()\n",
    "\n",
    "# Step 12: Residual Analysis\n",
    "# Calculate residuals to analyze prediction errors\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot the residuals\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(residuals, kde=True, color='green', bins=10, alpha=0.7)\n",
    "plt.title('Residual Analysis (Error Distribution)', fontsize=16)\n",
    "plt.xlabel('Residual Value (Actual - Predicted)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "\n",
    "# Step 13: Heatmap of Features\n",
    "# Correlation heatmap for the dataset\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation_matrix = df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Pairplot Visualization\n",
    "# Pairplot to show the relationships between features\n",
    "sns.pairplot(df, diag_kind='kde', palette='cool')\n",
    "plt.suptitle('Pairplot of House Features and Prices', y=1.02, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# or \n",
    "\n",
    "# Step 1: Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-Price-Prediction/master/USA_Housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 3: Display basic information\n",
    "print(\"Dataset Head:\\n\", df.head())\n",
    "print(\"\\nDataset Info:\\n\")\n",
    "print(df.info())\n",
    "print(\"\\nStatistical Summary:\\n\")\n",
    "print(df.describe())\n",
    "\n",
    "# Step 4: Check for missing values\n",
    "print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "# Step 5: Visualize the feature relationships\n",
    "sns.pairplot(df)\n",
    "plt.suptitle(\"Pairplot of Features\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Correlation heatmap\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Step 7: Define feature matrix (X) and target vector (y)\n",
    "features = ['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms', \n",
    "            'Avg. Area Number of Bedrooms', 'Area Population']\n",
    "X = df[features]\n",
    "y = df['Price']\n",
    "\n",
    "# Step 8: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "# Step 9: Train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 10: Model coefficients\n",
    "print(\"\\nModel Coefficients:\")\n",
    "for feature, coef in zip(features, model.coef_):\n",
    "    print(f\"{feature}: {coef:.2f}\")\n",
    "print(f\"Intercept: {model.intercept_:.2f}\")\n",
    "\n",
    "# Step 11: Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 12: Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Step 13: Plot Actual vs Predicted prices\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test, y_pred, color='green', alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Actual vs Predicted House Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals, kde=True, color='purple')\n",
    "plt.title('Distribution of Residuals')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc9084-f8ec-4310-9cc2-f173504fd9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To better target their marketing strategies, a real estate agency wants to classify houses \n",
    "into price categories: Low, Medium, and High. Build a Convolutional Neural Network \n",
    "(CNN) to classify houses based on the features provided in the dataset. '''\n",
    "\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 1: Load the Dataset\n",
    "url = \"https://raw.githubusercontent.com/huzaifsayed/Linear-Regression-Model-for-House-Price-Prediction/master/USA_Housing.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 2: Explore the data\n",
    "print(\"First 5 rows of data:\\n\", df.head())\n",
    "print(\"\\nData Info:\\n\")\n",
    "df.info()\n",
    "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
    "print(\"\\nStatistical Summary:\\n\", df.describe())\n",
    "\n",
    "# Step 3: Drop non-numeric columns\n",
    "df.drop('Address', axis=1, inplace=True)\n",
    "\n",
    "# Step 4: Create categorical price labels (Low, Medium, High) using quantiles\n",
    "df['PriceCategory'] = pd.qcut(df['Price'], q=3, labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Visualize category distribution\n",
    "sns.countplot(x=df['PriceCategory'])\n",
    "plt.title('Distribution of House Price Categories')\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Encode categorical labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['PriceCategoryEncoded'] = label_encoder.fit_transform(df['PriceCategory'])\n",
    "\n",
    "# Step 6: Define features and target\n",
    "X = df.drop(['Price', 'PriceCategory', 'PriceCategoryEncoded'], axis=1).values\n",
    "y = to_categorical(df['PriceCategoryEncoded'])  # One-hot encoding\n",
    "\n",
    "# Step 7: Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 8: Reshape input for CNN: (samples, height, width, channels)\n",
    "X_cnn = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1, 1)\n",
    "\n",
    "# Step 9: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cnn, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Adjusted CNN model for small input dimensions\n",
    "model = Sequential([\n",
    "    Conv2D(64, kernel_size=(1,1), activation='relu', input_shape=(X_train.shape[1], 1, 1)),  # Smaller kernel\n",
    "    MaxPooling2D(pool_size=(1,1)),  # Smaller pool size\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Conv2D(32, kernel_size=(1,1), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(1,1)),  # Smaller pool size\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 3 categories: Low, Medium, High\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Step 11: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=16, validation_split=0.1)\n",
    "\n",
    "\n",
    "# Step 12: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"\\n✅ Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Step 13: Plot training history\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title(\"Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title(\"Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 14: Classification Report and Confusion Matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7a028-6474-4bb7-9253-0a986cfe9726",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  A financial analyst wants to model house price trends over increasing area numbers of \n",
    "rooms. Use an LSTM-based Recurrent Neural Network to predict the next house price \n",
    "value based on historical patterns in sorted data.  '''\n",
    "\n",
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# 1. Data Preprocessing\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'Area': [1500, 1800, 2400, 3000, 3500, 4000, 4500, 5000, 5500, 6000],\n",
    "    'Rooms': [3, 4, 4, 5, 5, 6, 6, 7, 7, 8],\n",
    "    'Price': [400000, 450000, 500000, 600000, 700000, 800000, 900000, 1000000, 1100000, 1200000]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sorting by the number of rooms or area to observe trends over increasing area and room numbers\n",
    "df = df.sort_values(by=['Area', 'Rooms'])\n",
    "\n",
    "# Features: 'Area', 'Rooms'\n",
    "X = df[['Area', 'Rooms']].values\n",
    "\n",
    "# Target: 'Price'\n",
    "y = df['Price'].values\n",
    "\n",
    "# Normalize the features and target variable\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale X (area, rooms) and y (price)\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# 2. Create Sequences for LSTM\n",
    "def create_dataset(X, y, window_size=3):\n",
    "    X_data, y_data = [], []\n",
    "    for i in range(len(X) - window_size):\n",
    "        X_data.append(X[i:i + window_size])\n",
    "        y_data.append(y[i + window_size])\n",
    "    return np.array(X_data), np.array(y_data)\n",
    "\n",
    "# Define window size (this should be smaller than the data length)\n",
    "window_size = 3  # Set to a value smaller than your dataset length\n",
    "\n",
    "# Create sequences (X, y) for training\n",
    "X_seq, y_seq = create_dataset(X_scaled, y_scaled, window_size)\n",
    "\n",
    "# 3. Train/Test Split\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 4. Build LSTM Model\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# LSTM layer with 50 units\n",
    "model.add(LSTM(units=50, return_sequences=False, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "\n",
    "# Dense layer for output (house price prediction)\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 5. Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# 6. Evaluate the Model\n",
    "# Predict house prices on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse scaling of predictions and actual values to get prices in original scale\n",
    "y_pred_actual = scaler_y.inverse_transform(y_pred)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# 7. Results Visualization\n",
    "\n",
    "# Plotting training loss over epochs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM Model - Training & Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting actual vs predicted prices\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test_actual, label='Actual Prices', color='blue')\n",
    "plt.plot(y_pred_actual, label='Predicted Prices', color='red')\n",
    "plt.title('House Price Prediction - LSTM')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('House Price')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 8. Model Evaluation - RMSE\n",
    "# Calculate RMSE\n",
    "rmse = math.sqrt(mean_squared_error(y_test_actual, y_pred_actual))\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
    "\n",
    "# 9. Future Price Prediction\n",
    "\n",
    "# Get last sequence from training set (shape: [3, 2])\n",
    "last_sequence = X_train[-1]\n",
    "\n",
    "# Scale the last sequence properly\n",
    "last_sequence_scaled = scaler_X.transform(last_sequence)\n",
    "\n",
    "# Reshape to fit LSTM input shape [samples, time steps, features]\n",
    "new_sequence = np.reshape(last_sequence_scaled, (1, window_size, 2))\n",
    "\n",
    "# Predict the house price\n",
    "predicted_price_scaled = model.predict(new_sequence)\n",
    "\n",
    "# Inverse transform the prediction using target (y) scaler\n",
    "predicted_price = scaler_y.inverse_transform(predicted_price_scaled)\n",
    "\n",
    "print(\"Predicted Price:\", predicted_price[0][0])\n",
    "\n",
    "# Step 10 – Make Predictions and Plot\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "# Inverse scale predictions\n",
    "y_pred_actual = scaler_y.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Inverse scale y_test as well for comparison\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Extract the corresponding Area and Rooms from X_test for plotting\n",
    "# IMPORTANT: X_test shape is (samples, window_size, num_features)\n",
    "# So we need to extract the **last time step** features for each sequence\n",
    "areas_test = X_test[:, -1, 1]  # Area was the 2nd feature (index 1)\n",
    "rooms_test = X_test[:, -1, 0]  # Rooms was the 1st feature (index 0)\n",
    "\n",
    "# Sanity check: All must be of the same shape\n",
    "print(\"Shapes:\")\n",
    "print(\"Areas Test:\", areas_test.shape)\n",
    "print(\"Rooms Test:\", rooms_test.shape)\n",
    "print(\"y_pred_actual:\", y_pred_actual.shape)\n",
    "\n",
    "# Plotting: Predicted Prices vs Area\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(areas_test, y_pred_actual, color='red', label='Predicted Prices (LSTM)')\n",
    "plt.title('Predicted House Price vs. Area (LSTM)')\n",
    "plt.xlabel('Area (sq. ft.)')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting: Predicted Prices vs Rooms\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(rooms_test, y_pred_actual, color='green', label='Predicted Prices (LSTM)')\n",
    "plt.title('Predicted House Price vs. Rooms (LSTM)')\n",
    "plt.xlabel('Number of Rooms')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6811a53-5e71-4498-9243-c4fe173ae3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To streamline the approval of housing loans, a bank wants to classify houses into either \n",
    "\"High-value\" or \"Low-value\" categories. Implement a CNN model and optimize \n",
    "hyperparameters such as the number of layers, filter size, dropout, and learning rate to \n",
    "improve classification accuracy. ''' \n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "import shutil\n",
    "\n",
    "# ========== STEP 1: SIMULATE HOUSING DATA ==========\n",
    "def create_simulated_housing_dataset(base_dir, num_images=200):\n",
    "    if os.path.exists(base_dir):\n",
    "        shutil.rmtree(base_dir)  # Remove existing dataset\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    for label in ['high', 'low']:\n",
    "        class_dir = os.path.join(base_dir, label)\n",
    "        os.makedirs(class_dir, exist_ok=True)\n",
    "        for i in range(num_images):\n",
    "            if label == 'high':\n",
    "                # High-value houses: brighter color patterns\n",
    "                image_array = np.random.randint(150, 255, (128, 128, 3), dtype=np.uint8)\n",
    "            else:\n",
    "                # Low-value houses: duller colors\n",
    "                image_array = np.random.randint(0, 120, (128, 128, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(image_array)\n",
    "            img.save(os.path.join(class_dir, f\"{label}_{i}.jpg\"))\n",
    "\n",
    "data_dir = \"housing_dataset_simulated\"\n",
    "create_simulated_housing_dataset(data_dir, num_images=200)\n",
    "\n",
    "# ========== STEP 2: PREPARE DATA ==========\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_data = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_data = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(128, 128),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ========== STEP 3: BUILD CNN WITH TUNING ==========\n",
    "def build_model(hp):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.InputLayer((128, 128, 3)))\n",
    "\n",
    "    # Tune number of convolutional layers\n",
    "    for i in range(hp.Int('num_conv_layers', 1, 3)):\n",
    "        model.add(layers.Conv2D(\n",
    "            filters=hp.Choice(f'filters_{i}', values=[32, 64, 128]),\n",
    "            kernel_size=hp.Choice(f'kernel_size_{i}', values=[3, 5]),\n",
    "            activation='relu',\n",
    "            padding='same'\n",
    "        ))\n",
    "        model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Tune fully connected layer\n",
    "    model.add(layers.Dense(\n",
    "        units=hp.Int('dense_units', 64, 256, step=64),\n",
    "        activation='relu'\n",
    "    ))\n",
    "\n",
    "    # Tune dropout rate\n",
    "    model.add(layers.Dropout(rate=hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Tune learning rate\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Float('lr', 1e-4, 1e-2, sampling='LOG')\n",
    "        ),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# ========== STEP 4: HYPERPARAMETER TUNING ==========\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=1,\n",
    "    directory='housing_tuning',\n",
    "    project_name='housing_cnn'\n",
    ")\n",
    "\n",
    "print(\"\\n🔍 Starting hyperparameter search...\")\n",
    "tuner.search(train_data, validation_data=val_data, epochs=5)\n",
    "\n",
    "# ========== STEP 5: EVALUATE BEST MODEL ==========\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "print(\"\\n✅ Best Hyperparameters Found:\")\n",
    "for key, val in best_hp.values.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "\n",
    "# Optional: Train more on best model\n",
    "print(\"\\n🚀 Retraining best model...\")\n",
    "history = best_model.fit(train_data, validation_data=val_data, epochs=10)\n",
    "\n",
    "# Final evaluation\n",
    "test_loss, test_acc = best_model.evaluate(val_data)\n",
    "print(f\"\\n📊 Final Validation Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "best_model.save(\"house_value_classifier.h5\")\n",
    "print(\"\\n💾 Model saved as 'house_value_classifier.h5'\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb9854-9574-43a6-8abc-a19b192917e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A property review site wants to predict user sentiment (positive or negative) based on \n",
    "their written reviews. Build a sentiment classification model using an RNN on simulated \n",
    "review text data.'''\n",
    "\n",
    "\n",
    "# 📦 Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 📝 Simulated Review Dataset\n",
    "data = {\n",
    "    \"review\": [\n",
    "        \"I loved the location and the view\",\n",
    "        \"Very noisy and dirty neighborhood\",\n",
    "        \"Spacious and well-maintained apartment\",\n",
    "        \"Terrible service and old building\",\n",
    "        \"Beautiful interiors and helpful staff\",\n",
    "        \"Worst place I have stayed in\",\n",
    "        \"Clean rooms and peaceful environment\",\n",
    "        \"Unclean bathrooms and rude manager\",\n",
    "        \"Amazing place to stay with family\",\n",
    "        \"I will never recommend this property\"\n",
    "    ],\n",
    "    \"sentiment\": [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]  # 1 = Positive, 0 = Negative\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 🔧 Preprocessing: Tokenization and Padding\n",
    "vocab_size = 1000\n",
    "max_length = 10\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df['review'])\n",
    "sequences = tokenizer.texts_to_sequences(df['review'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "labels = np.array(df['sentiment'])\n",
    "\n",
    "# 🔀 Split into Train and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 🧠 Build RNN Model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=16, input_length=max_length),\n",
    "    SimpleRNN(32),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# 🏋️ Train Model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# 📈 Plot Accuracy & Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 🔍 Sentiment Prediction Function\n",
    "def predict_review_sentiment(text):\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(seq, maxlen=max_length, padding='post')\n",
    "    prediction = model.predict(padded)[0][0]\n",
    "    sentiment = \"Positive\" if prediction >= 0.5 else \"Negative\"\n",
    "    return sentiment, prediction\n",
    "\n",
    "# 🧪 Test Example\n",
    "test_text = \"The apartment was clean and spacious\"\n",
    "sentiment, score = predict_review_sentiment(test_text)\n",
    "print(f\"\\nReview: \\\"{test_text}\\\"\\nPredicted Sentiment: {sentiment} (Score: {score:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d3744-23c3-420f-b39d-242792929164",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''An analyst needs to compile data from various sources such as Excel files and SQL \n",
    "databases into a single data frame for further processing. Write a program to import data \n",
    "from Excel and a SQL server into a common format for unified analysis. '''\n",
    "\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# ---------------------------\n",
    "# Step 1: Load Excel File\n",
    "# ---------------------------\n",
    "excel_path = \"your_excel_file.xlsx\"  # Replace with your file path\n",
    "excel_df = pd.read_excel(excel_path)\n",
    "\n",
    "print(\"✅ Excel data loaded successfully\")\n",
    "print(excel_df.head())\n",
    "\n",
    "# ---------------------------\n",
    "# Step 2: Connect to SQL Server\n",
    "# ---------------------------\n",
    "# Option A: SQL Server Authentication\n",
    "conn_str = (\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=localhost\\\\SQLEXPRESS;\"     # Replace with your server name\n",
    "    \"DATABASE=YourDatabaseName;\"        # Replace with your database name\n",
    "    \"UID=your_username;\"                # Replace with your SQL username\n",
    "    \"PWD=your_password\"                 # Replace with your password\n",
    ")\n",
    "\n",
    "# Option B: Windows Authentication (Uncomment to use)\n",
    "# conn_str = (\n",
    "#     \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "#     \"SERVER=localhost\\\\SQLEXPRESS;\"\n",
    "#     \"DATABASE=YourDatabaseName;\"\n",
    "#     \"Trusted_Connection=yes;\"\n",
    "# )\n",
    "\n",
    "try:\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    print(\"✅ Connected to SQL Server\")\n",
    "\n",
    "    # ---------------------------\n",
    "    # Step 3: Query Data from SQL\n",
    "    # ---------------------------\n",
    "    sql_query = \"SELECT * FROM YourTableName\"  # Replace with your table name\n",
    "    sql_df = pd.read_sql(sql_query, conn)\n",
    "    print(\"✅ SQL data loaded successfully\")\n",
    "    print(sql_df.head())\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"❌ SQL Connection failed:\", e)\n",
    "\n",
    "# ---------------------------\n",
    "# Step 4: Merge/Concatenate Both DataFrames\n",
    "# ---------------------------\n",
    "# Option A: Concatenate vertically (same schema)\n",
    "# unified_df = pd.concat([excel_df, sql_df], ignore_index=True)\n",
    "\n",
    "# Option B: Merge by a common key (example: 'ID')\n",
    "# unified_df = pd.merge(excel_df, sql_df, on='ID', how='outer')\n",
    "\n",
    "# For now, we’ll just display both\n",
    "print(\"\\n📊 Excel Data:\")\n",
    "print(excel_df.head())\n",
    "\n",
    "print(\"\\n📊 SQL Data:\")\n",
    "print(sql_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b39a28-aa12-436c-8ccd-6909351bd674",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' A data engineer needs to extract housing data, transform it by creating new attributes \n",
    "(e.g., price per room), and load it into a relational database for reporting in Power BI. \n",
    "Simulate this ETL process using SQLite. '''\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Extract (Simulated data)\n",
    "# -------------------------------\n",
    "data = {\n",
    "    'HouseID': [1, 2, 3, 4, 5],\n",
    "    'Location': ['CityA', 'CityB', 'CityC', 'CityA', 'CityB'],\n",
    "    'Price': [500000, 600000, 550000, 700000, 650000],\n",
    "    'Rooms': [5, 4, 3, 6, 5],\n",
    "    'Area': [2000, 1800, 1500, 2200, 2100]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"✅ Extracted Data:\")\n",
    "print(df)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 2: Transform (Add new features/attributes)\n",
    "# -----------------------------------------------\n",
    "df['PricePerRoom'] = df['Price'] / df['Rooms']\n",
    "df['PricePerSqFt'] = df['Price'] / df['Area']\n",
    "\n",
    "print(\"\\n🔁 Transformed Data (with new attributes):\")\n",
    "print(df)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load into SQLite DB\n",
    "# -------------------------------\n",
    "# Connect to SQLite (or create a new one)\n",
    "conn = sqlite3.connect(\"housing_data.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table (if it doesn't exist)\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS Housing (\n",
    "    HouseID INTEGER PRIMARY KEY,\n",
    "    Location TEXT,\n",
    "    Price REAL,\n",
    "    Rooms INTEGER,\n",
    "    Area REAL,\n",
    "    PricePerRoom REAL,\n",
    "    PricePerSqFt REAL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Load DataFrame into SQLite\n",
    "df.to_sql(\"Housing\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"\\n✅ Data loaded into SQLite database 'housing_data.db' in table 'Housing'.\")\n",
    "\n",
    "# Optional: Verify inserted data\n",
    "print(\"\\n🔍 Sample data from DB:\")\n",
    "result = pd.read_sql_query(\"SELECT * FROM Housing\", conn)\n",
    "print(result)\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9657cb-aa03-4581-911c-425962ce8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    A real estate investment firm needs Excel-based dashboards for summarizing average \n",
    "income, price trends, and regional demographics. Export the housing dataset to Excel \n",
    "and use advanced Excel tools (Pivot Tables, Charts, Formulas) for visualization and \n",
    "reporting..'''\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Faker for realistic fake data\n",
    "fake = Faker()\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 1: Extract (Simulate raw housing dataset)\n",
    "# -----------------------------------------------\n",
    "def generate_housing_data(num_records=20):\n",
    "    data = {\n",
    "        'HouseID': [],\n",
    "        'OwnerName': [],\n",
    "        'Location': [],\n",
    "        'Price': [],\n",
    "        'Rooms': [],\n",
    "        'Area': [],\n",
    "        'YearBuilt': []\n",
    "    }\n",
    "    for i in range(1, num_records + 1):\n",
    "        data['HouseID'].append(i)\n",
    "        data['OwnerName'].append(fake.name())\n",
    "        data['Location'].append(random.choice(['Mumbai', 'Pune', 'Delhi', 'Bangalore', 'Hyderabad']))\n",
    "        price = random.randint(30_00_000, 90_00_000)\n",
    "        data['Price'].append(price)\n",
    "        rooms = random.randint(2, 6)\n",
    "        data['Rooms'].append(rooms)\n",
    "        area = random.randint(800, 3000)\n",
    "        data['Area'].append(area)\n",
    "        data['YearBuilt'].append(random.randint(1995, 2022))\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Simulate data\n",
    "df_raw = generate_housing_data()\n",
    "print(\"✅ Step 1: Extracted Raw Data\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 2: Transform (Add new features)\n",
    "# -----------------------------------------------\n",
    "def transform_data(df):\n",
    "    df['PricePerRoom'] = df['Price'] / df['Rooms']\n",
    "    df['PricePerSqFt'] = df['Price'] / df['Area']\n",
    "    df['HouseAge'] = 2025 - df['YearBuilt']\n",
    "    \n",
    "    # Optional: Clean or filter if needed (example)\n",
    "    df = df[df['Area'] > 1000]  # Keep only houses with area > 1000 sq ft\n",
    "    return df\n",
    "\n",
    "df_transformed = transform_data(df_raw)\n",
    "print(\"\\n🔁 Step 2: Transformed Data with New Attributes\")\n",
    "print(df_transformed.head())\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 3: Load (Insert into SQLite DB)\n",
    "# -----------------------------------------------\n",
    "def load_to_sqlite(df, db_name=\"housing_data_etl.db\", table_name=\"HousingData\"):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create table if it doesn't exist\n",
    "    cursor.execute(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "        HouseID INTEGER PRIMARY KEY,\n",
    "        OwnerName TEXT,\n",
    "        Location TEXT,\n",
    "        Price REAL,\n",
    "        Rooms INTEGER,\n",
    "        Area REAL,\n",
    "        YearBuilt INTEGER,\n",
    "        PricePerRoom REAL,\n",
    "        PricePerSqFt REAL,\n",
    "        HouseAge INTEGER\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Replace existing data\n",
    "    df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "    print(f\"\\n✅ Step 3: Data Loaded into SQLite DB '{db_name}' in Table '{table_name}'.\")\n",
    "\n",
    "    # Preview inserted records\n",
    "    preview = pd.read_sql_query(f\"SELECT * FROM {table_name} LIMIT 5\", conn)\n",
    "    print(\"\\n🔍 Preview of Data in DB:\")\n",
    "    print(preview)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Perform load\n",
    "load_to_sqlite(df_transformed)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Final Message\n",
    "# -----------------------------------------------\n",
    "print(\"\\n🎯 ETL Process Completed Successfully!\")\n",
    "print(\"You can now connect this database to Power BI for analysis and reporting.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e7bbfd-bda7-4008-98f8-de319b55114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  A real estate investment firm needs Excel-based dashboards for summarizing average \n",
    "income, price trends, and regional demographics. Export the housing dataset to Excel \n",
    "and use advanced Excel tools (Pivot Tables, Charts, Formulas) for visualization and \n",
    "reporting.  '''\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker\n",
    "import openpyxl\n",
    "\n",
    "# Initialize Faker for generating realistic fake data\n",
    "fake = Faker()\n",
    "\n",
    "# Step 1: Generate the housing data\n",
    "def generate_housing_data(num_records=100):\n",
    "    data = {\n",
    "        'HouseID': [],\n",
    "        'OwnerName': [],\n",
    "        'Location': [],\n",
    "        'Price': [],\n",
    "        'Rooms': [],\n",
    "        'Area': [],\n",
    "        'YearBuilt': [],\n",
    "        'Income': [],\n",
    "        'Demographic': [],\n",
    "        'Price_per_Room': []\n",
    "    }\n",
    "    \n",
    "    for i in range(1, num_records + 1):\n",
    "        data['HouseID'].append(i)\n",
    "        data['OwnerName'].append(fake.name())\n",
    "        data['Location'].append(random.choice(['Mumbai', 'Pune', 'Delhi', 'Bangalore', 'Hyderabad']))\n",
    "        price = random.randint(30_00_000, 90_00_000)\n",
    "        data['Price'].append(price)\n",
    "        rooms = random.randint(2, 6)\n",
    "        data['Rooms'].append(rooms)\n",
    "        area = random.randint(800, 3000)\n",
    "        data['Area'].append(area)\n",
    "        year_built = random.randint(1995, 2022)\n",
    "        data['YearBuilt'].append(year_built)\n",
    "        income = random.randint(5_00_000, 30_00_000)  # Simulating household income\n",
    "        data['Income'].append(income)\n",
    "        demographic = random.choice(['Urban', 'Suburban', 'Rural'])\n",
    "        data['Demographic'].append(demographic)\n",
    "        \n",
    "        # Price per Room calculation\n",
    "        price_per_room = price / rooms\n",
    "        data['Price_per_Room'].append(price_per_room)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate raw housing data\n",
    "df_raw = generate_housing_data(100)\n",
    "\n",
    "# Step 2: Export Data to Excel\n",
    "def export_to_excel(df, filename='datamf.xlsx'):\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"✅ Data exported to {filename}\")\n",
    "\n",
    "# Export the data\n",
    "export_to_excel(df_raw)\n",
    "\n",
    "# Now, let's move to Excel to manually create Pivot Tables and Visualizations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7d927-0e0f-4cab-91ae-46a41f16af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' To prioritize high-value listings, a real estate firm wants to predict whether a house falls \n",
    "in the \"High\" or \"Low\" price category. Build a Random Forest classifier to perform binary \n",
    "classification based on the house’s features'''\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Load the dataset (replace with your dataset)\n",
    "# Assuming the dataset is in CSV format\n",
    "# df = pd.read_csv(\"house_prices.csv\")\n",
    "\n",
    "# Example data: Replace with your own dataset\n",
    "data = {\n",
    "    'Bedrooms': [3, 2, 4, 3, 5, 4, 3, 2],\n",
    "    'Square_Feet': [1500, 1200, 2200, 1800, 2800, 2500, 2000, 1400],\n",
    "    'Location': ['Suburb', 'City', 'City', 'Suburb', 'City', 'Suburb', 'Suburb', 'City'],\n",
    "    'Price': [350000, 250000, 450000, 380000, 550000, 500000, 400000, 300000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Preprocessing\n",
    "# Convert categorical variable 'Location' to numerical using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['Location'] = le.fit_transform(df['Location'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop('Price', axis=1)  # Features\n",
    "y = df['Price']  # Target variable (house price)\n",
    "\n",
    "# Convert price to binary: High (1) or Low (0) based on median price\n",
    "median_price = df['Price'].median()\n",
    "y = y.apply(lambda price: 1 if price > median_price else 0)  # 1 for High, 0 for Low\n",
    "\n",
    "# Split data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Model Training\n",
    "# Create and train the Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Model Evaluation\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 5. Feature Importance (optional)\n",
    "feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                   index=X.columns,\n",
    "                                   columns=[\"Importance\"]).sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importances)\n",
    "\n",
    "# 6. Hyperparameter Tuning (using StratifiedKFold for cross-validation)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Use StratifiedKFold with cv=3 for cross-validation to handle class imbalance\n",
    "cv_strategy = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=cv_strategy, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters from grid search\n",
    "print(\"\\nBest parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train model again with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(f\"\\nBest Model Accuracy: {accuracy_score(y_test, y_pred_best)}\")\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411fdcbb-e126-4992-bc8f-513a8859be2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  To identify regional housing market segments, an analyst wants to group properties \n",
    "based on similar features. Apply the K-Means clustering algorithm to segment the \n",
    "housing dataset and visualize the clusters.'''\n",
    "\n",
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# 1. Loading the housing dataset (replace this with your actual dataset)\n",
    "# Assuming you have a DataFrame `df` with relevant features\n",
    "# df = pd.read_csv('housing_data.csv')\n",
    "\n",
    "# For demonstration purposes, let's generate a sample dataset with fewer than 10 rows\n",
    "data = {\n",
    "    'feature1': [2000, 1500, 1800, 2500, 2200, 2100, 2300, 2400],\n",
    "    'feature2': [3, 2, 3, 4, 4, 3, 4, 4],\n",
    "    'feature3': [1, 2, 1, 3, 2, 1, 3, 3]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Preprocessing: Scaling the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "\n",
    "# 3. Finding the Optimal Number of Clusters using Elbow Method\n",
    "inertias = []\n",
    "max_clusters = min(8, 10)  # Ensure the max number of clusters does not exceed the number of samples\n",
    "for i in range(1, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(scaled_features)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, max_clusters + 1), inertias, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# 4. Apply K-Means clustering with a chosen number of clusters (e.g., 3 clusters)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['Cluster'] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# 5. Visualizing the clusters (for 2D or 3D features)\n",
    "# In this case, we have 3 features, so we will reduce dimensions using PCA for visualization\n",
    "\n",
    "# 5.1 PCA for 2D Visualization\n",
    "pca = PCA(n_components=2)\n",
    "reduced_data = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Plot the clusters in 2D\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=df['Cluster'], cmap='viridis', s=100)\n",
    "plt.title('K-Means Clustering of Housing Data (2D)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# 5.2 3D Visualization (if there are more than two features)\n",
    "if df.shape[1] > 2:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    # Perform PCA to reduce to 3 components for 3D visualization\n",
    "    pca_3d = PCA(n_components=3)\n",
    "    reduced_data_3d = pca_3d.fit_transform(scaled_features)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    scatter = ax.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2], c=df['Cluster'], cmap='viridis')\n",
    "    ax.set_xlabel('PCA Component 1')\n",
    "    ax.set_ylabel('PCA Component 2')\n",
    "    ax.set_zlabel('PCA Component 3')\n",
    "    ax.set_title('K-Means Clustering of Housing Data (3D)')\n",
    "    fig.colorbar(scatter, label='Cluster')\n",
    "    plt.show()\n",
    "\n",
    "# 5.3 Cluster Centers Visualization\n",
    "centers = kmeans.cluster_centers_\n",
    "reduced_centers = pca.transform(centers)\n",
    "\n",
    "# Plot cluster centers in 2D\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=df['Cluster'], cmap='viridis', s=100)\n",
    "plt.scatter(reduced_centers[:, 0], reduced_centers[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "plt.title('K-Means Clustering with Centroids')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()\n",
    "\n",
    "# 6. Silhouette Score\n",
    "silhouette_avg = silhouette_score(scaled_features, df['Cluster'])\n",
    "print(f'Silhouette Score: {silhouette_avg:.2f}')\n",
    "\n",
    "# 7. Show the resulting dataframe with cluster labels\n",
    "print(df)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
